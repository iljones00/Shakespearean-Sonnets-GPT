{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4P4oaNpAW_P",
        "outputId": "4b1501c0-83f1-4aea-af2c-fb518bbccc14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 42.6 ms, sys: 109 ms, total: 151 ms\n",
            "Wall time: 4.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%%capture\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbXaVlvSAW_T"
      },
      "source": [
        "## Requirements Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr2jqNO8AW_V",
        "outputId": "6141efd6-4355-4481-ef13-0af28231e0ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 1.13.0+cu116\n"
          ]
        }
      ],
      "source": [
        "#Requirements Installation \n",
        "import os\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import zipfile\n",
        "import random\n",
        "import time\n",
        "import csv\n",
        "import datetime\n",
        "from itertools import compress\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n",
        "                         AdamW, get_linear_schedule_with_warmup, \\\n",
        "                         TrainingArguments, BeamScorer, Trainer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, \\\n",
        "                             RandomSampler, SequentialSampler\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL6BA_TPAW_V"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWan3AmsAW_W"
      },
      "outputs": [],
      "source": [
        "DEBUG           = False\n",
        "\n",
        "INPUT_DIR       = 'articles'\n",
        "\n",
        "USE_APEX        = True\n",
        "APEX_OPT_LEVEL  = 'O1'\n",
        "\n",
        "MODEL           = 'gpt2-medium' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n",
        "\n",
        "UNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n",
        "\n",
        "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
        "                    \"eos_token\": \"<|EOS|>\",\n",
        "                    \"unk_token\": \"<|UNK|>\",                    \n",
        "                    \"pad_token\": \"<|PAD|>\",\n",
        "                    \"sep_token\": \"<|SEP|>\"}\n",
        "                    \n",
        "MAXLEN          = 768  #{768, 1024, 1280, 1600}\n",
        "\n",
        "TRAIN_SIZE      = 0.9\n",
        "\n",
        "if USE_APEX:\n",
        "    TRAIN_BATCHSIZE = 2\n",
        "    BATCH_UPDATE    = 32\n",
        "    #TRAIN_BATCHSIZE = 2\n",
        "else:\n",
        "    TRAIN_BATCHSIZE = 2\n",
        "    BATCH_UPDATE    = 32\n",
        "\n",
        "EPOCHS          = 20\n",
        "LR              = 5e-3\n",
        "EPS             = 1e-8\n",
        "WARMUP_STEPS    = 1e2\n",
        "\n",
        "SEED            = 2020"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZilX0uiyAW_W"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm-SLMjRAW_X"
      },
      "source": [
        "##Clean Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPbwk-BiTorw",
        "outputId": "a896bde9-3915-48fa-a49a-9288e7cf7e50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfvYRcfoTxud"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/COMP SCI 496: Gen Deep Models Final Project/Sonnets.txt') as f:\n",
        "    lines = f.readlines()\n",
        "lines = ''.join(lines)\n",
        "lines = lines.split('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVECi8GjTLqC"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(stop_words='english', analyzer = 'word')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY1CcjJlXMIE",
        "outputId": "7c831edb-430e-45bc-df12-33d3a520044c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "154"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JGoLxuhWzXD"
      },
      "outputs": [],
      "source": [
        "keywords = []\n",
        "for i, line in enumerate(lines):\n",
        "  X = tfidf.fit_transform(lines[i].lower().split(' '))\n",
        "\n",
        "  feature_array = np.array(tfidf.get_feature_names())\n",
        "  tfidf_sorting = np.argsort(X.toarray()).flatten()[::-1]\n",
        "\n",
        "  n = 5\n",
        "  top_n = feature_array[tfidf_sorting][:n].tolist()\n",
        "  keywords.append(top_n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn5t7cQBZ1rN",
        "outputId": "ddce7a3b-391a-47fa-b7c1-a1c674259ff5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['fresh', 'fuel', 'decease', 'world', 'thee']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = tfidf.fit_transform(lines[0].lower().split(' '))\n",
        "\n",
        "feature_array = np.array(tfidf.get_feature_names())\n",
        "tfidf_sorting = np.argsort(X.toarray()).flatten()\n",
        "\n",
        "n = 5\n",
        "top_n = feature_array[tfidf_sorting][-n:].tolist()\n",
        "top_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMfJnHd5ZsWv",
        "outputId": "98509332-4c55-4528-f72b-5790c65d097c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['thee', 'world', 'decease', 'fuel', 'fresh']"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keywords[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRH4P45NAW_X"
      },
      "source": [
        "### Use Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGYtqB-AAW_Z",
        "outputId": "bf1f8a8f-2d55-4a7b-ab2d-a2b9debe614a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "154"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/COMP SCI 496: Gen Deep Models Final Project/Sonnets with keywords.txt') as f:\n",
        "    lines = f.readlines()\n",
        "lines = ''.join(lines)\n",
        "lines = lines.split('\\n\\n')\n",
        "len(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b6dLr5DAW_a",
        "outputId": "5bde7c34-5d92-4190-92de-1092b6ff6bd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['beauty, heir, memory, famine, grave',\n",
              " \"From fairest creatures we desire increase,\\nThat thereby beauty's rose might never die,\\nBut as the riper should by time decease,\\nHis tender heir might bear his memory:\\nBut thou contracted to thine own bright eyes,\\nFeed'st thy light's flame with self-substantial fuel,\\nMaking a famine where abundance lies,\\nThy self thy foe, to thy sweet self too cruel:\\nThou that art now the world's fresh ornament,\\nAnd only herald to the gaudy spring,\\nWithin thine own bud buriest thy content,\\nAnd, tender churl, mak'st waste in niggarding:\\nPity the world, or else this glutton be,\\nTo eat the world's due, by the grave and thee.\"]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[0].split('\\n', 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmaqPj3pAW_a",
        "outputId": "6d8fd06d-5075-45ac-f3ac-fa69929fc16d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of articles: 154\n",
            "CPU times: user 459 µs, sys: 0 ns, total: 459 µs\n",
            "Wall time: 525 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "data = {}       \n",
        "\n",
        "for i in range(len(lines)):\n",
        "    #id, category, title, keywords, text\n",
        "    id = i\n",
        "    keywords, text = lines[i].split('\\n', 1)\n",
        "    keywords = keywords.split(', ')\n",
        "    data[id] = [keywords, text]\n",
        "\n",
        "\n",
        "print(f\"Number of articles: {len(data) :,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxmACu0UAW_b",
        "outputId": "a7a79277-c117-4337-ae75-78721dbe3664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "152\n"
          ]
        }
      ],
      "source": [
        "for sonnet in data:\n",
        "    if len(data[sonnet][1].split('\\n')) != 14:\n",
        "        print(sonnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woaUw2FQAW_b"
      },
      "source": [
        "### Dataset and Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zscl6ZA2AW_c"
      },
      "outputs": [],
      "source": [
        "class myDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, tokenizer, randomize=True):\n",
        "\n",
        "        text, keywords = [], []\n",
        "        for k, v in data.items():\n",
        "            text.append(v[1])\n",
        "            keywords.append(v[0])\n",
        "\n",
        "        self.randomize = randomize\n",
        "        self.tokenizer = tokenizer \n",
        "        self.text      = text\n",
        "        self.keywords  = keywords  \n",
        "\n",
        "    #---------------------------------------------#\n",
        "\n",
        "    @staticmethod\n",
        "    def join_keywords(keywords, randomize=True):\n",
        "        N = len(keywords)\n",
        "\n",
        "        #random sampling and shuffle\n",
        "        if randomize: \n",
        "            M = random.choice(range(N+1))\n",
        "            keywords = keywords[:M]\n",
        "            random.shuffle(keywords)\n",
        "\n",
        "        return ','.join(keywords)\n",
        "\n",
        "    #---------------------------------------------#\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    #---------------------------------------------#\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        keywords = self.keywords[i].copy()\n",
        "        kw = self.join_keywords(keywords, self.randomize)\n",
        "        \n",
        "        input = SPECIAL_TOKENS['bos_token'] + kw + SPECIAL_TOKENS['sep_token'] + \\\n",
        "                self.text[i] + SPECIAL_TOKENS['eos_token']\n",
        "\n",
        "        encodings_dict = tokenizer(input,                                   \n",
        "                                   truncation=True, \n",
        "                                   max_length=MAXLEN, \n",
        "                                   padding=\"max_length\")   \n",
        "        \n",
        "        input_ids = encodings_dict['input_ids']\n",
        "        attention_mask = encodings_dict['attention_mask']\n",
        "        \n",
        "        return {'label': torch.tensor(input_ids),\n",
        "                'input_ids': torch.tensor(input_ids), \n",
        "                'attention_mask': torch.tensor(attention_mask)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgoX2xhfAW_c"
      },
      "outputs": [],
      "source": [
        "def split_data(data, S=TRAIN_SIZE):\n",
        "    # Shuffle ids\n",
        "    ids = list(data.keys())\n",
        "    random.shuffle(ids)\n",
        "\n",
        "    # Split into training and validation sets    \n",
        "    train_size = int(S * len(data))\n",
        "\n",
        "    train_ids = ids[:train_size]\n",
        "    val_ids = ids[train_size:]\n",
        "\n",
        "    train_data = dict()\n",
        "    for id in train_ids:\n",
        "        train_data[id] = data[id]\n",
        "\n",
        "    val_data = dict()\n",
        "    for id in val_ids:\n",
        "        val_data[id] = data[id]\n",
        "\n",
        "    return train_data, val_data \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TioRxK9lAW_d"
      },
      "source": [
        "## Loading Tokenizer, Config and Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaKPlVYMAW_d"
      },
      "outputs": [],
      "source": [
        "def get_tokenizer(special_tokens=None):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n",
        "\n",
        "    if special_tokens:\n",
        "        tokenizer.add_special_tokens(special_tokens)\n",
        "        print(\"Special tokens added\")\n",
        "    return tokenizer\n",
        "\n",
        "def get_model(tokenizer, special_tokens=None, load_model_path=None):\n",
        "\n",
        "    #GPT2LMHeadModel\n",
        "    if special_tokens:\n",
        "        config = AutoConfig.from_pretrained(MODEL, \n",
        "                                            bos_token_id=tokenizer.bos_token_id,\n",
        "                                            eos_token_id=tokenizer.eos_token_id,\n",
        "                                            sep_token_id=tokenizer.sep_token_id,\n",
        "                                            pad_token_id=tokenizer.pad_token_id,\n",
        "                                            output_hidden_states=False)\n",
        "    else: \n",
        "        config = AutoConfig.from_pretrained(MODEL,                                     \n",
        "                                            pad_token_id=tokenizer.eos_token_id,\n",
        "                                            output_hidden_states=False)    \n",
        "\n",
        "    #----------------------------------------------------------------#\n",
        "    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config)\n",
        "\n",
        "    if special_tokens:\n",
        "        #Special tokens added, model needs to be resized accordingly\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if load_model_path:\n",
        "        model.load_state_dict(torch.load(load_model_path))\n",
        "\n",
        "    model.cuda()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uprXpxx4AW_d",
        "outputId": "256faed1-1310-4c92-f17a-d698fbca2c11"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-medium\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-medium\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Assigning <|BOS|> to the bos_token key of the tokenizer\n",
            "Assigning <|EOS|> to the eos_token key of the tokenizer\n",
            "Assigning <|UNK|> to the unk_token key of the tokenizer\n",
            "Assigning <|PAD|> to the pad_token key of the tokenizer\n",
            "Assigning <|SEP|> to the sep_token key of the tokenizer\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Special tokens added\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-medium\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50258,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"pad_token_id\": 50260,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"sep_token_id\": 50261,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 5.82 s, sys: 2.3 s, total: 8.12 s\n",
            "Wall time: 16.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "tokenizer = get_tokenizer(special_tokens=SPECIAL_TOKENS)\n",
        "model = get_model(tokenizer, \n",
        "                  special_tokens=SPECIAL_TOKENS,\n",
        "                #   load_model_path='pytorch_model.bin'\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA79IXAeAW_e"
      },
      "outputs": [],
      "source": [
        "# - Freeze selective layers:\n",
        "# - Freeze all layers except last n:\n",
        "for parameter in model.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "for i, m in enumerate(model.transformer.h):        \n",
        "    #Only un-freeze the last n transformer blocks\n",
        "    if i+1 > 12 - UNFREEZE_LAST_N:\n",
        "        for parameter in m.parameters():\n",
        "            parameter.requires_grad = True \n",
        "\n",
        "for parameter in model.transformer.ln_f.parameters():        \n",
        "    parameter.requires_grad = True\n",
        "\n",
        "for parameter in model.lm_head.parameters():        \n",
        "    parameter.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S45S6kzhAW_e",
        "outputId": "44718795-7f00-41a3-9002-8f6fadd5b36f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'label': tensor([50257, 23701, 50261,  3792,   340, 11906,   481,    11, 11906,  2939,\n",
            "          815,  1394,  1280,   198,  3666,  4334, 29708,  2340,   284,   262,\n",
            "        34730,  1755,    30,   198,    35,   455, 14210,  6227,   616,  1017,\n",
            "        17024,   815,   307,  5445,    11,   198,  3633, 16187,   588,   284,\n",
            "        17903,   466, 15290,   616,  6504,    30,   198,  3792,   340, 11906,\n",
            "         4437,   326, 14210,  3758,   338,    83,   422, 17903,   198,  2396,\n",
            "         1290,   422,  1363,   656,   616, 23777,   284,   279,   563,    11,\n",
            "          198,  2514,  1064,   503,   427,  1047,   290, 21696,  2250,   287,\n",
            "          502,    11,   198,   464,  8354,   290,  3478,   273,   286, 11906,\n",
            "        35394,    30,   198,    46,    11,   645,     0, 11906,  1842,    11,\n",
            "          996,   881,    11,   318,   407,   523,  1049,    25,   198,  1026,\n",
            "          318,   616,  1842,   326,  7622,  6164,  4151, 21693,    25,   198,\n",
            "        24461,   898,  2081,  1842,   326,   288,   849,   616,  1334,  7433,\n",
            "           11,   198,  2514,   711,   262,  2342,   805,  1683,   329, 11906,\n",
            "        11060,    25,   198,  1890, 17903,  2342,   314,    11, 14590, 14210,\n",
            "          288,   455,  7765,  8057,    11,   198,  4863,   502,  1290,   572,\n",
            "           11,   351,  1854,   477,  1165,  1474,    13, 50258, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260]), 'input_ids': tensor([50257, 23701, 50261,  3792,   340, 11906,   481,    11, 11906,  2939,\n",
            "          815,  1394,  1280,   198,  3666,  4334, 29708,  2340,   284,   262,\n",
            "        34730,  1755,    30,   198,    35,   455, 14210,  6227,   616,  1017,\n",
            "        17024,   815,   307,  5445,    11,   198,  3633, 16187,   588,   284,\n",
            "        17903,   466, 15290,   616,  6504,    30,   198,  3792,   340, 11906,\n",
            "         4437,   326, 14210,  3758,   338,    83,   422, 17903,   198,  2396,\n",
            "         1290,   422,  1363,   656,   616, 23777,   284,   279,   563,    11,\n",
            "          198,  2514,  1064,   503,   427,  1047,   290, 21696,  2250,   287,\n",
            "          502,    11,   198,   464,  8354,   290,  3478,   273,   286, 11906,\n",
            "        35394,    30,   198,    46,    11,   645,     0, 11906,  1842,    11,\n",
            "          996,   881,    11,   318,   407,   523,  1049,    25,   198,  1026,\n",
            "          318,   616,  1842,   326,  7622,  6164,  4151, 21693,    25,   198,\n",
            "        24461,   898,  2081,  1842,   326,   288,   849,   616,  1334,  7433,\n",
            "           11,   198,  2514,   711,   262,  2342,   805,  1683,   329, 11906,\n",
            "        11060,    25,   198,  1890, 17903,  2342,   314,    11, 14590, 14210,\n",
            "          288,   455,  7765,  8057,    11,   198,  4863,   502,  1290,   572,\n",
            "           11,   351,  1854,   477,  1165,  1474,    13, 50258, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260,\n",
            "        50260, 50260, 50260, 50260, 50260, 50260, 50260, 50260]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'There are 138 samples for training, and 16 samples for validation testing'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data, val_data = split_data(data)\n",
        "\n",
        "train_dataset = myDataset(train_data, tokenizer)\n",
        "val_dataset = myDataset(val_data, tokenizer, randomize=False)\n",
        "\n",
        "print(train_dataset[0])\n",
        "f'There are {len(train_dataset) :,} samples for training, and {len(val_dataset) :,} samples for validation testing'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqfH1Rw6AW_e"
      },
      "source": [
        "## Finetune GPT-2 using Trainer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w9kDg0HTAW_f",
        "outputId": "2a1cd69b-8f97-4f47-cbae-6a3fa60c82b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using cuda_amp half precision backend\n",
            "***** Running training *****\n",
            "  Num examples = 138\n",
            "  Num Epochs = 20\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 32\n",
            "  Total optimization steps = 40\n",
            "  Number of trainable parameters = 253009920\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 12:26, Epoch 19/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>25.735849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>25.735849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>30.652786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>4.018051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>6.492215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>19.363756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.651411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.509197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.239179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>13.338900</td>\n",
              "      <td>1.109463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>13.338900</td>\n",
              "      <td>1.061289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>13.338900</td>\n",
              "      <td>1.039311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>13.338900</td>\n",
              "      <td>1.044285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>13.338900</td>\n",
              "      <td>1.019529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>13.338900</td>\n",
              "      <td>1.023037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>13.338900</td>\n",
              "      <td>1.074778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>13.338900</td>\n",
              "      <td>1.035129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>13.338900</td>\n",
              "      <td>1.180411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>13.338900</td>\n",
              "      <td>1.174446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.831000</td>\n",
              "      <td>1.283819</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to /content/\n",
            "Configuration saved in /content/config.json\n",
            "Model weights saved in /content/pytorch_model.bin\n",
            "tokenizer config file saved in /content/tokenizer_config.json\n",
            "Special tokens file saved in /content/special_tokens_map.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 11min 3s, sys: 1min 36s, total: 12min 39s\n",
            "Wall time: 12min 50s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/\",\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=TRAIN_BATCHSIZE,\n",
        "    per_device_eval_batch_size=TRAIN_BATCHSIZE,\n",
        "    gradient_accumulation_steps=BATCH_UPDATE,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    fp16_opt_level=APEX_OPT_LEVEL,\n",
        "    warmup_steps=WARMUP_STEPS,    \n",
        "    learning_rate=LR,\n",
        "    adam_epsilon=EPS,\n",
        "    weight_decay=0.01,        \n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=False,\n",
        "    logging_steps=20    \n",
        ")\n",
        "\n",
        "#---------------------------------------------------#\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,    \n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "#---------------------------------------------------#\n",
        "trainer.train()\n",
        "trainer.save_model()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ta6ODtteAW_f"
      },
      "outputs": [],
      "source": [
        "# Save to G-Drive ----------------------------------#\n",
        "!cp -r 'pytorch_model.bin' '/content/drive/MyDrive/Colab Notebooks/pytorch_model_V2.bin'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZkBYLm0AW_f"
      },
      "source": [
        "## Generating Text with Fine-tuned GPT-2 Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VzXaGvFqAW_f"
      },
      "outputs": [],
      "source": [
        "# !cp -r '/content/drive/MyDrive/Colab Notebooks/Text Generation/pytorch_model_V2.bin' 'pytorch_model.bin' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L5VSCV3EAW_g",
        "outputId": "34a25286-4bfc-423c-b8da-3eb251448a5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-medium\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-medium\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Assigning <|BOS|> to the bos_token key of the tokenizer\n",
            "Assigning <|EOS|> to the eos_token key of the tokenizer\n",
            "Assigning <|UNK|> to the unk_token key of the tokenizer\n",
            "Assigning <|PAD|> to the pad_token key of the tokenizer\n",
            "Assigning <|SEP|> to the sep_token key of the tokenizer\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Special tokens added\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-medium\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50257,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50258,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"pad_token_id\": 50260,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"sep_token_id\": 50261,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = get_tokenizer(special_tokens=SPECIAL_TOKENS)\n",
        "model = get_model(tokenizer, \n",
        "                  special_tokens=SPECIAL_TOKENS,\n",
        "                  load_model_path='/content/pytorch_model.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KWYLleX4AW_g",
        "outputId": "b2fdbe8f-b5bb-4e6e-dcbf-25e01dd3801b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|BOS|>love,milk,nice,sacrifice,heart<|SEP|>\n",
            "tensor([[50257, 23205,    11, 25433,    74,    11, 44460,    11, 30584, 31932,\n",
            "            11, 11499, 50261]])\n"
          ]
        }
      ],
      "source": [
        "keywords = ['love', 'milk', 'nice', 'sacrifice', 'heart']\n",
        "kw = myDataset.join_keywords(keywords, randomize=False)\n",
        "\n",
        "prompt = SPECIAL_TOKENS['bos_token'] + kw + SPECIAL_TOKENS['sep_token']\n",
        "print(prompt)\n",
        "         \n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "print(generated)\n",
        "device = torch.device(\"cuda\")\n",
        "generated = generated.to(device)\n",
        "\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f732ALKDAW_g",
        "outputId": "ca543a3e-9c6c-4fbe-82af-88f6071c0225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1: So then do not despair\n",
            "When I have seen thee frown upon my deeds: yet be wise; thou know'st mine eye is wakened. Time and fortune will bear this out straight!  And when in doubt thy heart begins to groan with griefs-- 'tis folly so vile that it cannot go well.' Three winters cold were the summer of Love lived for love alone on his loving breast-razing days thus far from home... but three summers' pride had stol'.\n",
            "And now thine eyes are raven black as hellish night doth live ere long hence still green leaves can see through these wastes quite ill? Is't winter a joy where sweets meet each hour dyed gay'? Proud monarch how happy you make me think ye look day byday!'\n",
            "\n",
            "\n",
            "2: O! how I love thee so;\n",
            "I hate that which thou hast not had: yet do allow me this slander thus. 'Thou usurer' wilt be mine heir and my glory to triumph in joy? And though thy heart doth teach it is thine inward Muse's fault--  If such a soul live with others but as an eye hath taught them ''To shun', what shall they say of you now'?...How canst Love then thrive when all men abhor each other!'\n",
            "But let no unkind spirit abuse his power by accusing him ill or well-wresting; he may kill ye outright for sportive pleasure alone quite assuredly none knoweth best.'\n",
            "\n",
            "\n",
            "3: I hate to tell you this sweet truth;\n",
            "But I love what is most dear in your breast:  If it be not with me then mine eye well knows.... Dear heart! do thy worst and best wail that thou shouldst forsake the world's loving spite? And yet look whom tears stain when they see thee frown on aught so foul or vile as my verse doth rehearse now-- 'tis better for niggardly Muse' s sake ''Than live oaths of thine ever-fixed scorn'. Proud monarchs boast how happy their lives are lived by speaking ill about others,' but no such pride lies buried where beauty sits alone still dwellers.'\n",
            "\n",
            "\n",
            "4: So thou hast thy wish;\n",
            "For as a miser I will not be death's conquest:  If that fell arrest meet with my love being charged. Love knows no hatred nor hate holds it in esteem to kill me outright? O! 'tis better policy thus for thee live and die single--Than mine eye well may characterise thine hour of heart-wanting doom when eyes wide are turned away from heaven'.\n",
            "O fearful meditation now dear friend what poverty brings forth but joyous cheerfulness still doth preserve the day so idly... And yet this sorrow lends life eternal restful comfort alone sadly making dulls happy creatures woe shall dwell on these poor blenches who have lived such strife at your expense.'\n",
            "\n",
            "\n",
            "5: Let me not be unfathered in thy deeds;\n",
            "My heart mine eye hath no correspondence with the truth:  If thou turn back and do anything untutored tell my face thus. Love's scythe to pry is thine inward love telling tongue-tied lies o'erhand! And yet this I know thee well by now quite untrue doth lie--Thou truly art too fair for truest hearts of true minds?...Thus vainly thinking on things past ill that have nothing new taught it so false a rhyme as 'tis told hereon.'\n",
            "No such fool loves him who fears his worst day coming doomfully near nimble woe being hour tired from all alone waiting time outworn Time will kill both friend or foe outright when he best finds fame still young but waking night deafless silent shame holds sway over posterity ere long date set hooks where they live shall dwell untold till death brag how happy she was before her eyes fell asleep dreaming wild days hence sadly supposing these woes evermore arising every boughs westward though men say better words come daily unbred, nor mournful remembrance hold black grief dear unless sorrow brings forth eternal hate thrice perjured oath can make defence look sour even among friends assured!\n",
            "\n",
            "\n",
            "6: My love is as fair\n",
            "As any mother's child to be; yet mine eyes are raven black: And although my heart torments me with longing for that which now doth stay. 'Tis not enough then but sweetest niggard loves have stol'n from thee! Proud of this pride I scorn and blame the world so ill As in thy breast sits a grudge more keen than hate? For since whence hast thou forged hooks into thine eye  Wherein crooked wires do hang unused where buried men fight'? Is it conscience or nature at all born vainly blind--wanting nothing better nor less...For why should others think on what they see before them alone woe brings forth thus vile an end doom o'tO fearful meditation!'\n",
            "\n",
            "\n",
            "7: Thou fool! why dost thou forsake thy self\n",
            "To bear the cross of love? Is it not sinful to shun this disgraceful shame; 'tis for fear that others might steal thee away: yet they do so every day.  If ever then in my life I lose sight and despair at a sad distempered loss--... And weep afresh o'er with sorrows still shall find joy arising thither from grief's eternal doom-suiting hand alone... Dear heart friend what need is there now but death brag how you survive these blunting tears by ill masked hatefully knowing eyes thus withering Time will make black night outlive your youth quite making orphans more happy beouteous, if only sometime immortal men can see well or know better than me right fair tell ye sweet Love lies buried where mine eye hath lived speaking truth gladly untrue words say when thoughts come unbred call them prayers vainly intermixed, yea oft times their false compareO mourners should no dull thought nor forget his name dies as fast As any mother knows best needsLet him live even till we die young.\n",
            "\n",
            "\n",
            "8: But what's in the brain that keeps me from hate?\n",
            "Love is not love; it depends upon moods and frownSays 'not mine' with tongue-tied disdain: no! I do forgive thy trespass. Three winters cold have taken my heart away thus farMy absence hast thou endured alone to be wooed of thrallage or thine eye--Thou wilt bear all ill deeds well when others will bail thee out...  And yet this doth teach Love how sweet a kiss sounds nowAnd three summers hath taught summer on trial quite witlessHow can winter brag so long as men live for nought'? Time wastes life heretic still day by night hour dreaming o'.O cunning time saith ''I never saw such beauty!'\n",
            "So then ah flattery lies buried deep within his loving breast where worms dwellers sleep each instant.'\n",
            "\n",
            "\n",
            "9: Thy love is as tender now\n",
            "As when first it was my friend's pleasure to kiss; but since that time I have sworn thee fair: for thy gentle heart doth approve. Dear me then thou knowest well art cruel and kind-hearted! 'Tis better policy thus than folly or ill luck being mine in action--  If nature would make worms like weeds grow on lovers' eyes she might be thought wise even by thine eye alone? And yet this proved false her beauty lies buried deep within the earth quite unseen of men.'\n",
            "\n",
            "\n",
            "10: In loving thee as thy self doth now seem\n",
            "My love is thine; mine eye hath no correspondence with truth: my heart knows it lies in restful sleep. Love's not death to me nor lays upon trust the judgment of conscience--Thou art both beloved and hated by many men! 'Tis better that this false adulterate should live than be vile shamefully strumpeted alone still.'\n",
            "For wherefore say I thou wilt steal from us? Wherein forsworn do we forsake our loves quite so wilfulness will carry careless leave... And thus shall fortune find grace meet if ever she finds her pleasure desired  If any man may call on mercy slay him outright at random when he calls back again day after night-time o'ercharged like madman, Three winters cold have put a curse ''So flattery!' yet none well might think such strife had taken place before sun was born,'\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Top-p (nucleus) text generation (10 samples):\n",
        "sample_outputs = model.generate(generated, \n",
        "                                do_sample=True,   \n",
        "                                min_length=50, \n",
        "                                max_length=MAXLEN,\n",
        "                                top_k=30,                                 \n",
        "                                top_p=0.7,        \n",
        "                                temperature=0.9,\n",
        "                                repetition_penalty=2.0,\n",
        "                                num_return_sequences=10\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
        "    a = len(','.join(keywords))    \n",
        "    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Uh64qZPJAW_g",
        "outputId": "1f9812b5-06e9-4618-c345-99b39fb759c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1: Thy love is more than enough to make my heart groan;\n",
            "For then I think on thee, and weep afresh:\n",
            "And yet this sorrow doth not vex me so much,\n",
            "Nor mine eye well knows what conscience is,\n",
            "To say the least of thy worth.\n",
            "I have sworn thee fair, but thou know'st\n",
            "Thou art fairest in all men's eyes,\n",
            "And 'gainst thy self thine own sweet graces add a greater grief!\n",
            "O! none loves her that she fears will be wrongfully mistaking,\n",
            "The loss of whom she thinks best disposed wilt bear ill.\n",
            "Thus vainly thinking on thee, whilst others look elsewhere,\n",
            "My thoughts are as black as night, till heaven clears,\n",
            "And makes them see new faces every hour,\n",
            "As those mourning eyes which mourn for their loved one.\n",
            "\n",
            "\n",
            "2: No longer mourn for me when I am gone;\n",
            "I have left my love alone, and found a better state\n",
            "Than that which mine own heart hath in store:\n",
            "So thou know'st thy self to be woe,\n",
            "And will bear thine eye straight, no matter how it turns\n",
            "In loving spite of ill-wresting hands,\n",
            "When others' graces do themselves disgrace.\n",
            "O! none knows well what conscience is,\n",
            "Whose action is right or wrong,\n",
            "Who hateth thee whom thou gav'st with false compare,\n",
            "To shun the judgment of conscience, unbred,\n",
            "For fear of whose scythe doth mercy lie,\n",
            "Which sharp teeth cannot pry, yet can make tongue-tied speaking mad?\n",
            "But if they rightly say so, 'tis true,'\n",
            "They are fond on praise, though their tongues\n",
            "Do not grace every vulgar song.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Beam-search text generation:\n",
        "sample_outputs = model.generate(generated, \n",
        "                                do_sample=True,   \n",
        "                                max_length=MAXLEN,                                                      \n",
        "                                num_beams=5,\n",
        "                                repetition_penalty=10.0,\n",
        "                                early_stopping=True,      \n",
        "                                num_return_sequences=2\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
        "    a = len(','.join(keywords))    \n",
        "    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPZU6qI4AW_h"
      },
      "source": [
        "## Generating Text with Raw GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e-pHelNJAW_h",
        "outputId": "72d375e6-30d2-480c-9f11-015d8843f96c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-medium\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-medium\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2-medium\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gpt2-medium/snapshots/e852c9080bc759a01663acf5a828d95b261a9903/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = get_tokenizer()\n",
        "model = get_model(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B_uI8cHEAW_h",
        "outputId": "c17cc360-d76a-488e-b4ae-7a5c89602a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: Create a shakespearean sonnet for me:  I'm afraid I'll have to write it from memory, but here's the first draft.\n",
            "This is my version of Shakespeare's \"A Midsummer Night's Dream\" in its original form (with some minor changes).\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Create a shakespearean sonnet for me: \"\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "device = torch.device(\"cuda\")\n",
        "generated = generated.to(device)\n",
        "\n",
        "model.eval()\n",
        "sample_outputs = model.generate(generated, \n",
        "                                do_sample=True,   \n",
        "                                max_length=MAXLEN,                                                      \n",
        "                                num_beams=5,\n",
        "                                repetition_penalty=5.0,\n",
        "                                early_stopping=True,      \n",
        "                                num_return_sequences=1\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cXHQ6gXbrHMH",
        "outputId": "db5e2055-3656-4558-fbe2-69ea687d3565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: Generate a shakespearean sonnet that starts with: O! lest the world should task you to recite __________.\n",
            "\n",
            "I don't know about you, but I'm not going to be writing my own sonnets any time soon.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Generate a shakespearean sonnet that starts with: O! lest the world should task you to recite \"\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "device = torch.device(\"cuda\")\n",
        "generated = generated.to(device)\n",
        "\n",
        "model.eval()\n",
        "sample_outputs = model.generate(generated, \n",
        "                                do_sample=True,   \n",
        "                                max_length=MAXLEN,                                                      \n",
        "                                num_beams=5,\n",
        "                                repetition_penalty=5.0,\n",
        "                                early_stopping=True,      \n",
        "                                num_return_sequences=1\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "b43f9f8e05fe433f9fefa68ccaf020fb",
    "deepnote_persisted_session": {
      "createdAt": "2022-11-21T20:10:14.093Z"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}